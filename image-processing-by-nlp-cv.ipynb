{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1111676,"sourceType":"datasetVersion","datasetId":623289}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport tensorflow as tf\nfrom tqdm import tqdm\n\n#NLP Imports\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import Sequence\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.layers import Embedding, LSTM, add, Input, Dense, Dropout, Concatenate, Reshape, concatenate, Bidirectional\n\n#Model Creation Imports\nfrom tensorflow.keras.models import Sequential, Model\n\n#Computer Vision Imports\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\nfrom tensorflow.keras.applications import DenseNet201\n\n#Model Learning & Losses Imports\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n\n#Warning & Visualization Imports\nimport warnings\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom textwrap import wrap\n\nplt.rcParams['font.size'] = 12\nsns.set_style(\"dark\")\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-11T15:23:46.463965Z","iopub.execute_input":"2025-02-11T15:23:46.464296Z","iopub.status.idle":"2025-02-11T15:23:46.470550Z","shell.execute_reply.started":"2025-02-11T15:23:46.464272Z","shell.execute_reply":"2025-02-11T15:23:46.469832Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_path = '/kaggle/input/flickr8k/Images'\ndata = pd.read_csv(\"/kaggle/input/flickr8k/captions.txt\")\ndata.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T10:33:05.500297Z","iopub.execute_input":"2025-02-11T10:33:05.500602Z","iopub.status.idle":"2025-02-11T10:33:05.630999Z","shell.execute_reply.started":"2025-02-11T10:33:05.500580Z","shell.execute_reply":"2025-02-11T10:33:05.630180Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def readImage(path,img_size=224):\n    img = load_img(path,color_mode='rgb',target_size=(img_size,img_size))\n    img = img_to_array(img)\n    img = img/255\n\n    return img\n\ndef display_images(temp_df):\n    temp_df = temp_df.reset_index(drop=True)\n    plt.figure(figsize = (20, 20))\n    n = 0\n    for i in range(15):\n        n+=1\n        plt.subplot(5, 5, n)\n        plt.subplots_adjust(hspace = 0.7, wspace = 0.3)\n        image = readImage(f\"../input/flickr8k/Images/{temp_df.image[i]}\")\n        plt.imshow(image)\n        plt.title(\"\\n\".join(wrap(temp_df.caption[i], 20)))\n        plt.axis(\"off\")\n\ndisplay_images(data.sample(15))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T10:52:15.650089Z","iopub.execute_input":"2025-02-11T10:52:15.650423Z","iopub.status.idle":"2025-02-11T10:52:18.225968Z","shell.execute_reply.started":"2025-02-11T10:52:15.650397Z","shell.execute_reply":"2025-02-11T10:52:18.224947Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def text_preprocessing(data):\n    data['caption'] = data['caption'].apply(lambda x: x.lower())\n    data['caption'] = data['caption'].apply(lambda x: x.replace(\"[^A-Za-z]\",\"\"))\n    data['caption'] = data['caption'].apply(lambda x: x.replace(\"\\s+\",\" \"))\n    data['caption'] = data['caption'].apply(lambda x: \" \".join([word for word in x.split()if len(word)>1]))\n    data['caption'] = \"startseq \"+data['caption']+\" endseq\"\n\n    return data\n\ndata = text_preprocessing(data)\n\ncaptions = data['caption'].tolist()\n\ncaptions[:10]\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T11:01:29.438743Z","iopub.execute_input":"2025-02-11T11:01:29.439087Z","iopub.status.idle":"2025-02-11T11:01:29.570768Z","shell.execute_reply.started":"2025-02-11T11:01:29.439062Z","shell.execute_reply":"2025-02-11T11:01:29.569922Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(captions)\nvocab_size = len(tokenizer.word_index)+1\nmax_length = max(len(caption.split()) for caption in captions)\n\nimages = data['image'].unique().tolist()\nnimages = len(images)\n\nsplit_index = round(0.85*nimages)\ntrain_images = images[:split_index]\nval_images = images[split_index:]\n\ntrain = data[data['image'].isin(train_images)]\ntest = data[data['image'].isin(val_images)]\n\ntrain.reset_index(inplace=True,drop=True)\ntest.reset_index(inplace=True,drop=True)\n\ntokenizer.texts_to_sequences([captions[1]])[0]\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T14:04:57.086330Z","iopub.execute_input":"2025-02-11T14:04:57.086677Z","iopub.status.idle":"2025-02-11T14:04:57.637881Z","shell.execute_reply.started":"2025-02-11T14:04:57.086651Z","shell.execute_reply":"2025-02-11T14:04:57.637129Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = DenseNet201()\n\nfe = Model(inputs=model.input, outputs=model.layers[-2].output)\n\nimg_size = 224\nfeatures = {}\nfor image in tqdm(data['image'].unique().tolist()):\n    img = load_img(os.path.join(image_path,image),target_size=(img_size,img_size))\n    img = img_to_array(img)\n    img = img/255.\n    img = np.expand_dims(img,axis=0)\n    feature = fe.predict(img, verbose=0)\n    features[image] = feature","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T13:38:03.466295Z","iopub.execute_input":"2025-02-11T13:38:03.466600Z","iopub.status.idle":"2025-02-11T13:50:31.809297Z","shell.execute_reply.started":"2025-02-11T13:38:03.466577Z","shell.execute_reply":"2025-02-11T13:50:31.808519Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CustomDataGenerator(Sequence):\n\n    def __init__(self, df, X_col, y_col, batch_size, directory, tokenizer,\n                 vocab_size, max_length, features,shuffle=True):\n\n        self.df = df.copy()\n        self.X_col = X_col\n        self.y_col = y_col\n        self.directory = directory\n        self.batch_size = batch_size\n        self.tokenizer = tokenizer\n        self.vocab_size = vocab_size\n        self.max_length = max_length\n        self.features = features\n        self.shuffle = shuffle\n        self.n = len(self.df)\n\n    def on_epoch_end(self):\n        if self.shuffle:\n            self.df = self.df.sample(frac=1).reset_index(drop=True)\n\n    def __len__(self):\n        return self.n // self.batch_size\n\n    def __getitem__(self,index):\n\n        batch = self.df.iloc[index * self.batch_size:(index + 1) * self.batch_size,:]\n        X1, X2, y = self.__get_data(batch)\n        return (X1, X2), y\n\n    def __get_data(self,batch):\n\n        X1, X2, y = list(), list(), list()\n\n        images = batch[self.X_col].tolist()\n\n        for image in images:\n            feature = self.features[image][0]\n\n            captions = batch.loc[batch[self.X_col]==image, self.y_col].tolist()\n            for caption in captions:\n                seq = self.tokenizer.texts_to_sequences([caption])[0]\n\n                for i in range(1,len(seq)):\n                    in_seq, out_seq = seq[:i], seq[i]\n                    in_seq = pad_sequences([in_seq], maxlen=self.max_length)[0]\n                    out_seq = to_categorical([out_seq], num_classes=self.vocab_size)[0]\n                    X1.append(feature)\n                    X2.append(in_seq)\n                    y.append(out_seq)\n\n        X1, X2, y = np.array(X1), np.array(X2), np.array(y)\n\n        return X1, X2, y\n\ntrain_generator = CustomDataGenerator(df=train,X_col='image',y_col='caption',batch_size=64,directory=image_path,\n                                      tokenizer=tokenizer,vocab_size=vocab_size,max_length=max_length,features=features)\n\nvalidation_generator = CustomDataGenerator(df=test,X_col='image',y_col='caption',batch_size=64,directory=image_path,\n                                      tokenizer=tokenizer,vocab_size=vocab_size,max_length=max_length,features=features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T14:10:28.888972Z","iopub.execute_input":"2025-02-11T14:10:28.889302Z","iopub.status.idle":"2025-02-11T14:10:28.903204Z","shell.execute_reply.started":"2025-02-11T14:10:28.889281Z","shell.execute_reply":"2025-02-11T14:10:28.902337Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_generator[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T14:10:34.153248Z","iopub.execute_input":"2025-02-11T14:10:34.153580Z","iopub.status.idle":"2025-02-11T14:10:34.624830Z","shell.execute_reply.started":"2025-02-11T14:10:34.153550Z","shell.execute_reply":"2025-02-11T14:10:34.623923Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.utils import plot_model\n\ninput1 = Input(shape=(1920,))\ninput2 = Input(shape=(max_length,))\n\n\nimg_features = Dense(256, activation='relu')(input1)\nimg_features_reshaped = Reshape((1, 256), input_shape=(256,))(img_features)\n\nsentence_features = Embedding(vocab_size, 256, mask_zero=False)(input2)\n\nmerged = concatenate([img_features_reshaped,sentence_features],axis=1)\nsentence_features = LSTM(256)(merged)\n\nx = Dropout(0.5)(sentence_features)\nx = add([x, img_features])\n\nx = Dense(128, activation='relu')(x)\nx = Dropout(0.5)(x)\noutput = Dense(vocab_size, activation='softmax')(x)\n\ncaption_model = Model(inputs=[input1,input2], outputs=output)\ncaption_model.compile(loss='categorical_crossentropy',optimizer='adam')\n\n\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n\n#Define the model checkpoint\n\nmodel_name = \"model.keras\" #Update then extension to .keras\ncheckpoint = ModelCheckpoint(\n    model_name,\n    monitor=\"val_loss\",\n    mode=\"min\",\n    save_best_only=True,\n    save_weights_only=False,\n    verbose=1\n)\n\nearlystopping = EarlyStopping(monitor='val_loss',min_delta = 0, patience = 5, verbose = 1, restore_best_weights=True)\n\nlearning_rate_reduction = ReduceLROnPlateau(\n    monitor='val_loss',\n    patience=3,\n    verbose=1,\n    factor=0.2,\n     min_lr=0.00000001\n)\n\n\nhistory = caption_model.fit(\n        train_generator,\n        epochs=50,\n        validation_data=validation_generator,\n        callbacks=[checkpoint,earlystopping,learning_rate_reduction]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T15:23:53.251551Z","iopub.execute_input":"2025-02-11T15:23:53.251897Z","iopub.status.idle":"2025-02-11T15:42:43.355106Z","shell.execute_reply.started":"2025-02-11T15:23:53.251832Z","shell.execute_reply":"2025-02-11T15:42:43.354291Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(20,8))\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T15:45:39.125336Z","iopub.execute_input":"2025-02-11T15:45:39.125632Z","iopub.status.idle":"2025-02-11T15:45:39.456617Z","shell.execute_reply.started":"2025-02-11T15:45:39.125610Z","shell.execute_reply":"2025-02-11T15:45:39.455768Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pickle\n\n#Save the tokenizer\nwith open(\"tokenizer.pkl\",\"wb\") as f:\n    pickle.dump(tokenizer, f)\n\n#Save the feature extractor model\nfe.save(\"feature_extractor.keras\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T16:35:47.677178Z","iopub.execute_input":"2025-02-11T16:35:47.677521Z","iopub.status.idle":"2025-02-11T16:35:49.022994Z","shell.execute_reply.started":"2025-02-11T16:35:47.677495Z","shell.execute_reply":"2025-02-11T16:35:49.022083Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nimport matplotlib.pyplot as plt\nimport pickle\n\n#load save files\nmodel_path = \"model.keras\"\ntokenizer_path = \"tokenizer.pkl\"\nfeature_extractor_path = \"feature_extractor.keras\"\n\ndef generate_and_display_caption(image_path, model_path, tokenizer_path, feature_extractor_path,max_length=34,img_size=224):\n    caption_model = load_model(model_path)\n    feature_extractor = load_model(feature_extractor_path)\n    tokenizer = pickle.load(open(\"tokenizer.pkl\",'rb'))\n\n    #preprocess and read image\n    img = load_img(image_path, target_size=(img_size,img_size))\n    img = img_to_array(img) / 255.0 #0,1\n    img = np.expand_dims(img,axis=0)\n\n    image_features = feature_extractor.predict(img, verbose=0)\n\n    #generate caption for this new image\n    in_text = \"startseq\"\n    for i in range(max_length):\n        sequence = tokenizer.texts_to_sequences([in_text])[0]\n        sequence = pad_sequences([sequence], maxlen=max_length)\n        yhat = caption_model.predict([image_features,sequence], verbose=0)\n        yhat_index = np.argmax(yhat)\n        word = tokenizer.index_word.get(yhat_index, None)\n        if word is None:\n            break\n        in_text +=\" \"+ word\n        if word == 'endseq':\n            break\n        caption = in_text.replace('startseq','').replace(\"endseq\",'').strip()\n\n        #display image with caption\n        img = load_img(image_path, target_size=(img_size,img_size))\n        plt.figure(figsize=(8,8))\n        plt.imshow(img)\n        plt.axis('off')\n        plt.title(caption, fontsize=16, color='blue')\n        plt.show()\n        \n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T16:44:13.879437Z","iopub.execute_input":"2025-02-11T16:44:13.879773Z","iopub.status.idle":"2025-02-11T16:44:13.887469Z","shell.execute_reply.started":"2025-02-11T16:44:13.879746Z","shell.execute_reply":"2025-02-11T16:44:13.886652Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Example usage\nimage_path = \"/kaggle/input/flickr8k/Images/1000268201_693b08cb0e.jpg\" #Replace with the path to the input image\ngenerate_and_display_caption(image_path, model_path, tokenizer_path, feature_extractor_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T16:50:12.600317Z","iopub.execute_input":"2025-02-11T16:50:12.600669Z","iopub.status.idle":"2025-02-11T16:50:30.699978Z","shell.execute_reply.started":"2025-02-11T16:50:12.600644Z","shell.execute_reply":"2025-02-11T16:50:30.698954Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_path = \"/kaggle/input/flickr8k/Images/1007129816_e794419615.jpg\" #Replace with the path to the input image\ngenerate_and_display_caption(image_path, model_path, tokenizer_path, feature_extractor_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T16:51:56.489138Z","iopub.execute_input":"2025-02-11T16:51:56.489474Z","iopub.status.idle":"2025-02-11T16:52:14.268114Z","shell.execute_reply.started":"2025-02-11T16:51:56.489441Z","shell.execute_reply":"2025-02-11T16:52:14.267268Z"}},"outputs":[],"execution_count":null}]}